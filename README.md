# Author Profiling Using Support Vector Machines.

##First step



We start by compiling the files using:

		python3 compile.py path_to_files
for example:
		python3 compile.py ./PAN16/

PAN16 is a folder which contains all the files and resides in the same path of the file compile.py

This step works on converting the files from .xml format to .txt format producing a folder "compiled_dataset".


##Second step

		javac ReplaceSpecialStrings.java

		java ReplaceSpecialStrings

Which will take the .txt files generated by the first step and tag them using special tags as explained in the paper.

For example @username will be replaced by _MENTION_TAG.

It will produce a folder "tagged_dataset" which contains the tagged files.

##Third step

		python3 training.py ./tagged_dataset/ path_to_truth_file

This step will get the tagged files produced by the second step as an input and train a model and produce a .pkl model file.
the training phase makes use of the preparing module "preparing_module.py"

After training, a classification report will be produced. For example, a classification report after training using the PAN16 dataset:
		Detailed classification report:

		The model is trained on the full development set.
		The scores are computed on the full evaluation set.

        	precision    recall  f1-score   support

        		1       0.00      0.00      0.00         2
        		2       0.30      0.23      0.26        26
        		3       0.31      0.52      0.39        27
        		4       0.25      0.08      0.12        12
        		5       0.00      0.00      0.00         1
        		6       0.00      0.00      0.00         2
        		7       0.41      0.46      0.44        26
        		8       0.31      0.32      0.31        34
        		9       0.33      0.21      0.26        14

		avg / total       0.31      0.33      0.31       144


After finishing this phase we get a .pkl model file "model.pkl" which will be used later in the testing pahse.

##Fourth step

This phase we test our model which was produced in the third step. We can test the model on the same dataset which was used to generate it. This will make use of the preparing module for testing phase "preparing_module_testing.py" 

		python3 testing.py ./tagged_dataset/ ./model.pkl ./output/

The output directory will contain .xml files one file for each file of the dataset. For example: 

		<author id="{0a9e35fd6f123137d585a482f2484d8e}" type="twitter" lang="en" age_group="35-49" gender="male"/>

which is fortelling the age and the gender of the an author of some specific file. This format is suitable to do the evaluation of the tira test.

**Links:**
TIRA:
http://www.tira.io/
**PAN16:**
http://pan.webis.de/index.html
**CLEF16**
http://clef2016.clef-initiative.eu/




























